{
  "data": [
    {
      "slug": "openai/codex-mini",
      "hf_slug": "",
      "updated_at": "2025-05-16T16:10:30.488453+00:00",
      "created_at": "2025-05-16T15:36:01.081688+00:00",
      "hf_updated_at": null,
      "name": "OpenAI: Codex Mini",
      "short_name": "Codex Mini",
      "author": "openai",
      "description": "codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct use in the API, we recommend starting with gpt-4.1.",
      "model_version_group_id": null,
      "context_length": 200000,
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "has_text_output": true,
      "group": "GPT",
      "instruct_type": null,
      "default_system": null,
      "default_stops": [],
      "hidden": false,
      "router": null,
      "warning_message": null,
      "permaslug": "openai/codex-mini",
      "reasoning_config": null,
      "features": {},
      "endpoint": {
        "id": "421864ed-5bce-4ae5-b02e-7345b5878842",
        "name": "OpenAI | openai/codex-mini",
        "context_length": 200000,
        "model": {
          "slug": "openai/codex-mini",
          "hf_slug": "",
          "updated_at": "2025-05-16T16:10:30.488453+00:00",
          "created_at": "2025-05-16T15:36:01.081688+00:00",
          "hf_updated_at": null,
          "name": "OpenAI: Codex Mini",
          "short_name": "Codex Mini",
          "author": "openai",
          "description": "codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct use in the API, we recommend starting with gpt-4.1.",
          "model_version_group_id": null,
          "context_length": 200000,
          "input_modalities": [
            "image",
            "text"
          ],
          "output_modalities": [
            "text"
          ],
          "has_text_output": true,
          "group": "GPT",
          "instruct_type": null,
          "default_system": null,
          "default_stops": [],
          "hidden": false,
          "router": null,
          "warning_message": null,
          "permaslug": "openai/codex-mini",
          "reasoning_config": null,
          "features": {}
        },
        "model_variant_slug": "openai/codex-mini",
        "model_variant_permaslug": "openai/codex-mini",
        "provider_name": "OpenAI",
        "provider_info": {
          "name": "OpenAI",
          "displayName": "OpenAI",
          "slug": "openai",
          "baseUrl": "url",
          "dataPolicy": {
            "termsOfServiceURL": "https://openai.com/policies/row-terms-of-use/",
            "privacyPolicyURL": "https://openai.com/policies/privacy-policy/",
            "dataPolicyUrl": "https://platform.openai.com/docs/guides/your-data",
            "paidModels": {
              "training": false,
              "retainsPrompts": true,
              "retentionDays": 30
            },
            "requiresUserIDs": true
          },
          "headquarters": "US",
          "hasChatCompletions": true,
          "hasCompletions": true,
          "isAbortable": true,
          "moderationRequired": true,
          "group": "OpenAI",
          "editors": [],
          "owners": [],
          "isMultipartSupported": true,
          "statusPageUrl": "https://status.openai.com/",
          "byokEnabled": true,
          "isPrimaryProvider": true,
          "icon": {
            "url": "/images/icons/OpenAI.svg",
            "invertRequired": true
          }
        },
        "provider_display_name": "OpenAI",
        "provider_model_id": "codex-mini-latest",
        "provider_group": "OpenAI",
        "quantization": null,
        "variant": "standard",
        "is_free": false,
        "can_abort": true,
        "max_prompt_tokens": null,
        "max_completion_tokens": 100000,
        "max_prompt_images": null,
        "max_tokens_per_image": null,
        "supported_parameters": [
          "tools",
          "tool_choice",
          "max_tokens",
          "reasoning",
          "include_reasoning",
          "structured_outputs",
          "response_format",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "seed",
          "logit_bias",
          "logprobs",
          "top_logprobs"
        ],
        "is_byok": false,
        "moderation_required": true,
        "data_policy": {
          "termsOfServiceURL": "https://openai.com/policies/row-terms-of-use/",
          "privacyPolicyURL": "https://openai.com/policies/privacy-policy/",
          "dataPolicyUrl": "https://platform.openai.com/docs/guides/your-data",
          "paidModels": {
            "training": false,
            "retainsPrompts": true,
            "retentionDays": 30
          },
          "requiresUserIDs": true,
          "training": false,
          "retainsPrompts": true,
          "retentionDays": 30
        },
        "pricing": {
          "prompt": "0.0000015",
          "completion": "0.000006",
          "image": "0",
          "request": "0",
          "input_cache_read": "0.000000375",
          "web_search": "0",
          "internal_reasoning": "0",
          "discount": 0
        },
        "variable_pricings": [],
        "is_hidden": false,
        "is_deranked": false,
        "is_disabled": false,
        "supports_tool_parameters": true,
        "supports_reasoning": true,
        "supports_multipart": true,
        "limit_rpm": null,
        "limit_rpd": null,
        "has_completions": true,
        "has_chat_completions": true,
        "features": {
          "supported_parameters": {
            "response_format": true,
            "structured_outputs": true
          },
          "supports_document_url": null
        },
        "provider_region": null
      }
    },
    {
      "slug": "meta-llama/llama-3.3-8b-instruct",
      "hf_slug": "",
      "updated_at": "2025-05-16T15:08:09.272636+00:00",
      "created_at": "2025-05-14T13:42:34.22207+00:00",
      "hf_updated_at": null,
      "name": "Meta: Llama 3.3 8B Instruct (free)",
      "short_name": "Llama 3.3 8B Instruct (free)",
      "author": "meta-llama",
      "description": "A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.",
      "model_version_group_id": null,
      "context_length": 128000,
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "has_text_output": true,
      "group": "Llama3",
      "instruct_type": null,
      "default_system": null,
      "default_stops": [],
      "hidden": false,
      "router": null,
      "warning_message": null,
      "permaslug": "meta-llama/llama-3.3-8b-instruct",
      "reasoning_config": null,
      "features": {},
      "endpoint": {
        "id": "629d30c1-50eb-4172-985f-b2339176d2c9",
        "name": "Meta | meta-llama/llama-3.3-8b-instruct:free",
        "context_length": 128000,
        "model": {
          "slug": "meta-llama/llama-3.3-8b-instruct",
          "hf_slug": "",
          "updated_at": "2025-05-16T15:08:09.272636+00:00",
          "created_at": "2025-05-14T13:42:34.22207+00:00",
          "hf_updated_at": null,
          "name": "Meta: Llama 3.3 8B Instruct",
          "short_name": "Llama 3.3 8B Instruct",
          "author": "meta-llama",
          "description": "A lightweight and ultra-fast variant of Llama 3.3 70B, for use when quick response times are needed most.",
          "model_version_group_id": null,
          "context_length": 128000,
          "input_modalities": [
            "text"
          ],
          "output_modalities": [
            "text"
          ],
          "has_text_output": true,
          "group": "Llama3",
          "instruct_type": null,
          "default_system": null,
          "default_stops": [],
          "hidden": false,
          "router": null,
          "warning_message": null,
          "permaslug": "meta-llama/llama-3.3-8b-instruct",
          "reasoning_config": null,
          "features": {}
        },
        "model_variant_slug": "meta-llama/llama-3.3-8b-instruct:free",
        "model_variant_permaslug": "meta-llama/llama-3.3-8b-instruct:free",
        "provider_name": "Meta",
        "provider_info": {
          "name": "Meta",
          "displayName": "Meta",
          "slug": "meta",
          "baseUrl": "url",
          "dataPolicy": {
            "termsOfServiceURL": "https://ai.meta.com/legal/llama-api-terms/",
            "privacyPolicyURL": "https://www.facebook.com/privacy/policy/",
            "paidModels": {
              "training": false,
              "retainsPrompts": true,
              "retentionDays": 30
            }
          },
          "headquarters": "US",
          "hasChatCompletions": true,
          "hasCompletions": false,
          "isAbortable": false,
          "moderationRequired": false,
          "group": "Meta",
          "editors": [],
          "owners": [],
          "isMultipartSupported": true,
          "statusPageUrl": null,
          "byokEnabled": true,
          "isPrimaryProvider": true,
          "icon": {
            "url": "https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://ai.meta.com/&size=256"
          }
        },
        "provider_display_name": "Meta",
        "provider_model_id": "Llama-3.3-8B-Instruct",
        "provider_group": "Meta",
        "quantization": "fp8",
        "variant": "free",
        "is_free": true,
        "can_abort": false,
        "max_prompt_tokens": null,
        "max_completion_tokens": 4028,
        "max_prompt_images": null,
        "max_tokens_per_image": null,
        "supported_parameters": [
          "tools",
          "tool_choice",
          "max_tokens",
          "temperature",
          "top_p",
          "structured_outputs",
          "response_format",
          "repetition_penalty",
          "top_k"
        ],
        "is_byok": false,
        "moderation_required": false,
        "data_policy": {
          "termsOfServiceURL": "https://ai.meta.com/legal/llama-api-terms/",
          "privacyPolicyURL": "https://www.facebook.com/privacy/policy/",
          "paidModels": {
            "training": false,
            "retainsPrompts": true,
            "retentionDays": 30
          },
          "training": false,
          "retainsPrompts": true,
          "retentionDays": 30
        },
        "pricing": {
          "prompt": "0",
          "completion": "0",
          "image": "0",
          "request": "0",
          "web_search": "0",
          "internal_reasoning": "0",
          "discount": 0
        },
        "variable_pricings": [],
        "is_hidden": false,
        "is_deranked": false,
        "is_disabled": false,
        "supports_tool_parameters": true,
        "supports_reasoning": false,
        "supports_multipart": true,
        "limit_rpm": null,
        "limit_rpd": null,
        "has_completions": false,
        "has_chat_completions": true,
        "features": {
          "supported_parameters": {
            "response_format": true,
            "structured_outputs": true
          },
          "supports_document_url": null
        },
        "provider_region": null
      }
    },
    {
      "slug": "nousresearch/deephermes-3-mistral-24b-preview",
      "hf_slug": "NousResearch/DeepHermes-3-Mistral-24B-Preview",
      "updated_at": "2025-05-09T23:02:10.58499+00:00",
      "created_at": "2025-05-09T22:48:24.165453+00:00",
      "hf_updated_at": null,
      "name": "Nous: DeepHermes 3 Mistral 24B Preview (free)",
      "short_name": "DeepHermes 3 Mistral 24B Preview (free)",
      "author": "nousresearch",
      "description": "DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.\n\nDeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *\"deep thinking\"* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer. \n\nSystem Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n",
      "model_version_group_id": null,
      "context_length": 32768,
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "has_text_output": true,
      "group": "Other",
      "instruct_type": null,
      "default_system": null,
      "default_stops": [],
      "hidden": false,
      "router": null,
      "warning_message": null,
      "permaslug": "nousresearch/deephermes-3-mistral-24b-preview",
      "reasoning_config": null,
      "features": {},
      "endpoint": {
        "id": "90728530-42b4-4bcf-bc67-167eee5bbac4",
        "name": "Chutes | nousresearch/deephermes-3-mistral-24b-preview:free",
        "context_length": 32768,
        "model": {
          "slug": "nousresearch/deephermes-3-mistral-24b-preview",
          "hf_slug": "NousResearch/DeepHermes-3-Mistral-24B-Preview",
          "updated_at": "2025-05-09T23:02:10.58499+00:00",
          "created_at": "2025-05-09T22:48:24.165453+00:00",
          "hf_updated_at": null,
          "name": "Nous: DeepHermes 3 Mistral 24B Preview",
          "short_name": "DeepHermes 3 Mistral 24B Preview",
          "author": "nousresearch",
          "description": "DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.\n\nDeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *\"deep thinking\"* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer. \n\nSystem Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n",
          "model_version_group_id": null,
          "context_length": 32768,
          "input_modalities": [
            "text"
          ],
          "output_modalities": [
            "text"
          ],
          "has_text_output": true,
          "group": "Other",
          "instruct_type": null,
          "default_system": null,
          "default_stops": [],
          "hidden": false,
          "router": null,
          "warning_message": null,
          "permaslug": "nousresearch/deephermes-3-mistral-24b-preview",
          "reasoning_config": null,
          "features": {}
        },
        "model_variant_slug": "nousresearch/deephermes-3-mistral-24b-preview:free",
        "model_variant_permaslug": "nousresearch/deephermes-3-mistral-24b-preview:free",
        "provider_name": "Chutes",
        "provider_info": {
          "name": "Chutes",
          "displayName": "Chutes",
          "slug": "chutes",
          "baseUrl": "url",
          "dataPolicy": {
            "termsOfServiceURL": "https://chutes.ai/tos",
            "paidModels": {
              "training": true,
              "retainsPrompts": true
            }
          },
          "hasChatCompletions": true,
          "hasCompletions": true,
          "isAbortable": true,
          "moderationRequired": false,
          "group": "Chutes",
          "editors": [],
          "owners": [],
          "isMultipartSupported": true,
          "statusPageUrl": null,
          "byokEnabled": true,
          "isPrimaryProvider": true,
          "icon": {
            "url": "https://t0.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=https://chutes.ai/&size=256"
          }
        },
        "provider_display_name": "Chutes",
        "provider_model_id": "NousResearch/DeepHermes-3-Mistral-24B-Preview",
        "provider_group": "Chutes",
        "quantization": null,
        "variant": "free",
        "is_free": true,
        "can_abort": true,
        "max_prompt_tokens": null,
        "max_completion_tokens": null,
        "max_prompt_images": null,
        "max_tokens_per_image": null,
        "supported_parameters": [
          "max_tokens",
          "temperature",
          "top_p",
          "reasoning",
          "include_reasoning",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "seed",
          "top_k",
          "min_p",
          "repetition_penalty",
          "logprobs",
          "logit_bias",
          "top_logprobs"
        ],
        "is_byok": false,
        "moderation_required": false,
        "data_policy": {
          "termsOfServiceURL": "https://chutes.ai/tos",
          "paidModels": {
            "training": true,
            "retainsPrompts": true
          },
          "training": true,
          "retainsPrompts": true
        },
        "pricing": {
          "prompt": "0",
          "completion": "0",
          "image": "0",
          "request": "0",
          "web_search": "0",
          "internal_reasoning": "0",
          "discount": 0
        },
        "variable_pricings": [],
        "is_hidden": false,
        "is_deranked": false,
        "is_disabled": false,
        "supports_tool_parameters": false,
        "supports_reasoning": true,
        "supports_multipart": true,
        "limit_rpm": null,
        "limit_rpd": null,
        "has_completions": true,
        "has_chat_completions": true,
        "features": {
          "supported_parameters": {},
          "supports_document_url": null
        },
        "provider_region": null
      }
    },
    {
      "slug": "mistralai/mistral-medium-3",
      "hf_slug": "",
      "updated_at": "2025-05-07T14:30:43.39556+00:00",
      "created_at": "2025-05-07T14:15:41.980763+00:00",
      "hf_updated_at": null,
      "name": "Mistral: Mistral Medium 3",
      "short_name": "Mistral Medium 3",
      "author": "mistralai",
      "description": "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
      "model_version_group_id": null,
      "context_length": 131072,
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "has_text_output": true,
      "group": "Mistral",
      "instruct_type": null,
      "default_system": null,
      "default_stops": [],
      "hidden": false,
      "router": null,
      "warning_message": null,
      "permaslug": "mistralai/mistral-medium-3",
      "reasoning_config": null,
      "features": {},
      "endpoint": {
        "id": "9d5ba5bf-8465-46df-9185-1330820338f5",
        "name": "Mistral | mistralai/mistral-medium-3",
        "context_length": 131072,
        "model": {
          "slug": "mistralai/mistral-medium-3",
          "hf_slug": "",
          "updated_at": "2025-05-07T14:30:43.39556+00:00",
          "created_at": "2025-05-07T14:15:41.980763+00:00",
          "hf_updated_at": null,
          "name": "Mistral: Mistral Medium 3",
          "short_name": "Mistral Medium 3",
          "author": "mistralai",
          "description": "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
          "model_version_group_id": null,
          "context_length": 131072,
          "input_modalities": [
            "text",
            "image"
          ],
          "output_modalities": [
            "text"
          ],
          "has_text_output": true,
          "group": "Mistral",
          "instruct_type": null,
          "default_system": null,
          "default_stops": [],
          "hidden": false,
          "router": null,
          "warning_message": null,
          "permaslug": "mistralai/mistral-medium-3",
          "reasoning_config": null,
          "features": {}
        },
        "model_variant_slug": "mistralai/mistral-medium-3",
        "model_variant_permaslug": "mistralai/mistral-medium-3",
        "provider_name": "Mistral",
        "provider_info": {
          "name": "Mistral",
          "displayName": "Mistral",
          "slug": "mistral",
          "baseUrl": "url",
          "dataPolicy": {
            "termsOfServiceURL": "https://mistral.ai/terms/#terms-of-use",
            "privacyPolicyURL": "https://mistral.ai/terms/#privacy-policy",
            "paidModels": {
              "training": false,
              "retainsPrompts": true,
              "retentionDays": 30
            }
          },
          "headquarters": "FR",
          "hasChatCompletions": true,
          "hasCompletions": false,
          "isAbortable": false,
          "moderationRequired": false,
          "group": "Mistral",
          "editors": [],
          "owners": [],
          "isMultipartSupported": true,
          "statusPageUrl": null,
          "byokEnabled": true,
          "isPrimaryProvider": true,
          "icon": {
            "url": "/images/icons/Mistral.png"
          }
        },
        "provider_display_name": "Mistral",
        "provider_model_id": "mistral-medium-2505",
        "provider_group": "Mistral",
        "quantization": null,
        "variant": "standard",
        "is_free": false,
        "can_abort": false,
        "max_prompt_tokens": null,
        "max_completion_tokens": null,
        "max_prompt_images": null,
        "max_tokens_per_image": null,
        "supported_parameters": [
          "tools",
          "tool_choice",
          "max_tokens",
          "temperature",
          "top_p",
          "stop",
          "frequency_penalty",
          "presence_penalty",
          "response_format",
          "structured_outputs",
          "seed"
        ],
        "is_byok": false,
        "moderation_required": false,
        "data_policy": {
          "termsOfServiceURL": "https://mistral.ai/terms/#terms-of-use",
          "privacyPolicyURL": "https://mistral.ai/terms/#privacy-policy",
          "paidModels": {
            "training": false,
            "retainsPrompts": true,
            "retentionDays": 30
          },
          "training": false,
          "retainsPrompts": true,
          "retentionDays": 30
        },
        "pricing": {
          "prompt": "0.0000004",
          "completion": "0.000002",
          "image": "0",
          "request": "0",
          "web_search": "0",
          "internal_reasoning": "0",
          "discount": 0
        },
        "variable_pricings": [],
        "is_hidden": false,
        "is_deranked": false,
        "is_disabled": false,
        "supports_tool_parameters": true,
        "supports_reasoning": false,
        "supports_multipart": true,
        "limit_rpm": null,
        "limit_rpd": null,
        "has_completions": false,
        "has_chat_completions": true,
        "features": {
          "supported_parameters": {},
          "supports_document_url": null
        },
        "provider_region": null
      }
    }
  ]
}