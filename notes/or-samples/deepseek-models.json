[
  {
    "slug": "deepseek/deepseek-r1-0528",
    "hf_slug": "deepseek-ai/DeepSeek-R1-0528",
    "updated_at": "2025-05-28T19:32:43.989388+00:00",
    "created_at": "2025-05-28T17:59:30.833128+00:00",
    "name": "DeepSeek: R1 0528 (free)",
    "short_name": "R1 0528 (free)",
    "author": "deepseek",
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": "deepseek-r1",
    "warning_message": null,
    "permaslug": "deepseek/deepseek-r1-0528",
    "reasoning_config": {
      "start_token": "<think>",
      "end_token": "</think>"
    }
  },
  {
    "slug": "deepseek/deepseek-r1-0528",
    "hf_slug": "deepseek-ai/DeepSeek-R1-0528",
    "updated_at": "2025-05-28T19:32:43.989388+00:00",
    "created_at": "2025-05-28T17:59:30.833128+00:00",
    "name": "DeepSeek: R1 0528",
    "short_name": "R1 0528",
    "author": "deepseek",
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    "context_length": 128000,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": "deepseek-r1",
    "warning_message": null,
    "permaslug": "deepseek/deepseek-r1-0528",
    "reasoning_config": {
      "start_token": "<think>",
      "end_token": "</think>"
    }
  },
  {
    "slug": "deepseek/deepseek-prover-v2",
    "hf_slug": "deepseek-ai/DeepSeek-Prover-V2-671B",
    "updated_at": "2025-04-30T13:01:50.865169+00:00",
    "created_at": "2025-04-30T11:38:14.302503+00:00",
    "name": "DeepSeek: DeepSeek Prover V2 (free)",
    "short_name": "DeepSeek Prover V2 (free)",
    "author": "deepseek",
    "description": "DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-prover-v2",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-prover-v2",
    "hf_slug": "deepseek-ai/DeepSeek-Prover-V2-671B",
    "updated_at": "2025-04-30T13:01:50.865169+00:00",
    "created_at": "2025-04-30T11:38:14.302503+00:00",
    "name": "DeepSeek: DeepSeek Prover V2",
    "short_name": "DeepSeek Prover V2",
    "author": "deepseek",
    "description": "DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.",
    "context_length": 131072,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-prover-v2",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-v3-base",
    "hf_slug": "deepseek-ai/Deepseek-v3-base",
    "updated_at": "2025-03-29T18:19:47.595899+00:00",
    "created_at": "2025-03-29T18:13:43.339174+00:00",
    "name": "DeepSeek: DeepSeek V3 Base (free)",
    "short_name": "DeepSeek V3 Base (free)",
    "author": "deepseek",
    "description": "Note that this is a base model mostly meant for testing, you need to provide detailed prompts for the model to return useful responses. \n\nDeepSeek-V3 Base is a 671B parameter open Mixture-of-Experts (MoE) language model with 37B active parameters per forward pass and a context length of 128K tokens. Trained on 14.8T tokens using FP8 mixed precision, it achieves high training efficiency and stability, with strong performance across language, reasoning, math, and coding tasks. \n\nDeepSeek-V3 Base is the pre-trained model behind [DeepSeek V3](/deepseek/deepseek-chat-v3)",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-v3-base",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-chat-v3-0324",
    "hf_slug": "deepseek-ai/DeepSeek-V3-0324",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2025-03-24T13:59:15.252028+00:00",
    "name": "DeepSeek: DeepSeek V3 0324 (free)",
    "short_name": "DeepSeek V3 0324 (free)",
    "author": "deepseek",
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-chat-v3-0324",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-chat-v3-0324",
    "hf_slug": "deepseek-ai/DeepSeek-V3-0324",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2025-03-24T13:59:15.252028+00:00",
    "name": "DeepSeek: DeepSeek V3 0324",
    "short_name": "DeepSeek V3 0324",
    "author": "deepseek",
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-chat-v3-0324",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-r1-zero",
    "hf_slug": "deepseek-ai/DeepSeek-R1-Zero",
    "updated_at": "2025-05-02T21:14:16.355933+00:00",
    "created_at": "2025-03-06T21:43:54+00:00",
    "name": "DeepSeek: DeepSeek R1 Zero (free)",
    "short_name": "DeepSeek R1 Zero (free)",
    "author": "deepseek",
    "description": "DeepSeek-R1-Zero is a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step. It's 671B parameters in size, with 37B active in an inference pass.\n\nIt demonstrates remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\n\nDeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. See [DeepSeek R1](/deepseek/deepseek-r1) for the SFT model.\n\n",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "Other",
    "instruct_type": "deepseek-r1",
    "warning_message": null,
    "permaslug": "deepseek/deepseek-r1-zero",
    "reasoning_config": {
      "start_token": "<think>",
      "end_token": "</think>"
    }
  },
  {
    "slug": "deepseek/deepseek-r1",
    "hf_slug": "deepseek-ai/DeepSeek-R1",
    "updated_at": "2025-05-28T18:12:14.285209+00:00",
    "created_at": "2025-01-20T13:51:35.96912+00:00",
    "name": "DeepSeek: R1 (free)",
    "short_name": "R1 (free)",
    "author": "deepseek",
    "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": "deepseek-r1",
    "warning_message": null,
    "permaslug": "deepseek/deepseek-r1",
    "reasoning_config": {
      "start_token": "<think>",
      "end_token": "</think>"
    }
  },
  {
    "slug": "deepseek/deepseek-r1",
    "hf_slug": "deepseek-ai/DeepSeek-R1",
    "updated_at": "2025-05-28T18:12:14.285209+00:00",
    "created_at": "2025-01-20T13:51:35.96912+00:00",
    "name": "DeepSeek: R1",
    "short_name": "R1",
    "author": "deepseek",
    "description": "DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model & [technical report](https://api-docs.deepseek.com/news/news250120).\n\nMIT licensed: Distill & commercialize freely!",
    "context_length": 128000,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": "deepseek-r1",
    "warning_message": null,
    "permaslug": "deepseek/deepseek-r1",
    "reasoning_config": {
      "start_token": "<think>",
      "end_token": "</think>"
    }
  },
  {
    "slug": "deepseek/deepseek-chat",
    "hf_slug": "deepseek-ai/DeepSeek-V3",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2024-12-26T19:28:40.559917+00:00",
    "name": "DeepSeek: DeepSeek V3 (free)",
    "short_name": "DeepSeek V3 (free)",
    "author": "deepseek",
    "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-chat-v3",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-chat",
    "hf_slug": "deepseek-ai/DeepSeek-V3",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2024-12-26T19:28:40.559917+00:00",
    "name": "DeepSeek: DeepSeek V3",
    "short_name": "DeepSeek V3",
    "author": "deepseek",
    "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.\n\nFor model details, please visit [the DeepSeek-V3 repo](https://github.com/deepseek-ai/DeepSeek-V3) for more information, or see the [launch announcement](https://api-docs.deepseek.com/news/news1226).",
    "context_length": 163840,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "DeepSeek",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-chat-v3",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-chat-v2.5",
    "hf_slug": "deepseek-ai/DeepSeek-V2.5",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2024-05-14T00:00:00+00:00",
    "name": "DeepSeek V2.5",
    "short_name": "DeepSeek V2.5",
    "author": "deepseek",
    "description": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct. The new model integrates the general and coding abilities of the two previous versions. For model details, please visit [DeepSeek-V2 page](https://github.com/deepseek-ai/DeepSeek-V2) for more information.",
    "context_length": 128000,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "Other",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-chat",
    "reasoning_config": null
  },
  {
    "slug": "deepseek/deepseek-coder",
    "hf_slug": "deepseek-ai/DeepSeek-Coder-V2-Instruct",
    "updated_at": "2025-03-28T03:20:30.853469+00:00",
    "created_at": "2024-05-14T00:00:00+00:00",
    "name": "DeepSeek-Coder-V2",
    "short_name": "DeepSeek-Coder-V2",
    "author": "deepseek",
    "description": "DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model. It is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens.\n\nThe original V1 model was trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. It was pre-trained on project-level code corpus by employing a extra fill-in-the-blank task.",
    "context_length": 128000,
    "input_modalities": [
      "text"
    ],
    "output_modalities": [
      "text"
    ],
    "group": "Other",
    "instruct_type": null,
    "warning_message": null,
    "permaslug": "deepseek/deepseek-coder",
    "reasoning_config": null
  }
]