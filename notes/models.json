[
  {
    "id": "nousresearch/hermes-3-llama-3.1-405b",
    "name": "Nous: Hermes 3 405B Instruct",
    "created": 1723766400,
    "description": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\n\nHermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\n\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\n\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.",
    "architecture": {
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Llama3",
      "instruct_type": "chatml",
      "modality": "text->text"
    },
    "top_provider": {
      "is_moderated": false,
      "context_length": 131072,
      "max_completion_tokens": 131072
    },
    "pricing": {
      "prompt": "0.0000008",
      "completion": "0.0000008",
      "image": "0",
      "request": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "context_length": 131072,
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "temperature",
      "top_p",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "seed",
      "logit_bias",
      "logprobs",
      "top_logprobs",
      "response_format",
      "top_k",
      "repetition_penalty",
      "min_p"
    ]
  },
  {
    "id": "nousresearch/deephermes-3-mistral-24b-preview:free",
    "name": "Nous: DeepHermes 3 Mistral 24B Preview (free)",
    "created": 1746830904,
    "description": "DeepHermes 3 (Mistral 24B Preview) is an instruction-tuned language model by Nous Research based on Mistral-Small-24B, designed for chat, function calling, and advanced multi-turn reasoning. It introduces a dual-mode system that toggles between intuitive chat responses and structured “deep reasoning” mode using special system prompts. Fine-tuned via distillation from R1, it supports structured output (JSON mode) and function call syntax for agent-based applications.\n\nDeepHermes 3 supports a **reasoning toggle via system prompt**, allowing users to switch between fast, intuitive responses and deliberate, multi-step reasoning. When activated with the following specific system instruction, the model enters a *\"deep thinking\"* mode—generating extended chains of thought wrapped in `<think></think>` tags before delivering a final answer. \n\nSystem Prompt: You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside <think> </think> tags, and then provide your solution or response to the problem.\n",
    "architecture": {
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Other",
      "instruct_type": null,
      "modality": "text->text"
    },
    "top_provider": {
      "is_moderated": false,
      "context_length": 32768,
      "max_completion_tokens": null
    },
    "pricing": {
      "prompt": "0",
      "completion": "0",
      "image": "0",
      "request": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "context_length": 32768,
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "temperature",
      "top_p",
      "reasoning",
      "include_reasoning",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "seed",
      "top_k",
      "min_p",
      "repetition_penalty",
      "logprobs",
      "logit_bias",
      "top_logprobs"
    ]
  },
  {
    "id": "mistralai/mistral-medium-3",
    "name": "Mistral: Mistral Medium 3",
    "created": 1746627341,
    "description": "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
    "architecture": {
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null,
      "modality": "text+image->text"
    },
    "top_provider": {
      "is_moderated": false,
      "context_length": 131072,
      "max_completion_tokens": null
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.000002",
      "image": "0",
      "request": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "context_length": 131072,
    "per_request_limits": null,
    "supported_parameters": [
      "tools",
      "tool_choice",
      "max_tokens",
      "temperature",
      "top_p",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "response_format",
      "structured_outputs",
      "seed"
    ]
  },
  {
    "id": "google/gemini-2.5-pro-preview",
    "name": "Google: Gemini 2.5 Pro Preview",
    "created": 1746578513,
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "architecture": {
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null,
      "modality": "text+image->text"
    },
    "top_provider": {
      "is_moderated": false,
      "context_length": 1048576,
      "max_completion_tokens": 65535
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "image": "0.00516",
      "request": "0",
      "input_cache_read": "0.00000031",
      "input_cache_write": "0.000001625",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "context_length": 1048576,
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "temperature",
      "top_p",
      "tools",
      "tool_choice",
      "stop",
      "seed",
      "response_format",
      "structured_outputs"
    ]
  },
  {
    "id": "arcee-ai/caller-large",
    "name": "Arcee AI: Caller Large",
    "created": 1746487869,
    "description": "Caller Large is Arcee's specialist \"function‑calling\" SLM built to orchestrate external tools and APIs. Instead of maximizing next‑token accuracy, training focuses on structured JSON outputs, parameter extraction and multi‑step tool chains, making Caller a natural choice for retrieval‑augmented generation, robotic process automation or data‑pull chatbots. It incorporates a routing head that decides when (and how) to invoke a tool versus answering directly, reducing hallucinated calls. The model is already the backbone of Arcee Conductor's auto‑tool mode, where it parses user intent, emits clean function signatures and hands control back once the tool response is ready. Developers thus gain an OpenAI‑style function‑calling UX without handing requests to a frontier‑scale model. ",
    "architecture": {
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Other",
      "instruct_type": null,
      "modality": "text->text"
    },
    "top_provider": {
      "is_moderated": false,
      "context_length": 32768,
      "max_completion_tokens": null
    },
    "pricing": {
      "prompt": "0.00000055",
      "completion": "0.00000085",
      "image": "0",
      "request": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "context_length": 32768,
    "per_request_limits": null,
    "supported_parameters": [
      "tools",
      "tool_choice",
      "max_tokens",
      "temperature",
      "top_p",
      "stop",
      "frequency_penalty",
      "presence_penalty",
      "top_k",
      "repetition_penalty",
      "logit_bias",
      "min_p",
      "response_format"
    ]
  }
]